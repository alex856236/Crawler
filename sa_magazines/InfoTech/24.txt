機器人 懂你心 : 在我們跟機器人一起生活之前，必須先教它們了解並模仿人類情緒。研發人員致力機器學習，試圖為機器人裝上「一顆心」，好讓它們成為我們的生活良伴。
重點提要■當我們越來越常跟語音和手勢控制的機器互動，就會期望它們能辨識情緒，了解高階溝通的特徵，例如幽默、挖苦和意圖。■為了讓這樣的溝通成真，我們必須幫機器裝上同理心模組，這是一種軟體系統，能從人類的語音和行為中擷取出情緒線索，然後引導機器做出相對反應。■同理心機器人的研究才剛起步，但科學家已經利用訊號處理技術、機器學習演算法和情緒分析工具，建構能「了解」人類情緒的虛擬機器人。
「抱歉，我沒有聽到你說話。」
這或許是商用機器第一次說出有同理心的言語。1990年代晚期，美國波士頓的語音成果國際公司（SpeechWorksInternational）提供企業一套客服軟體，內建許多句子，其中就包含這句話。在那之後，我們漸漸習慣跟機器講話，幾乎每通打進客服專線的電話，都是先跟機器對話。如今幾億人的口袋裡都有智慧型個人助理，我們可以請Siri或其他類似軟體找餐廳、打電話給朋友或找出歌曲播放，它們甚至能模擬古怪的人類行為。（人類：「Siri，你愛我嗎？」Siri：「我沒有愛的能力。」）
但機器的反應並非都如我們預期，語音辨識軟體會出錯。機器經常無法了解人類的意圖，包括情緒、幽默、挖苦和反諷。如果未來我們會花更多時間跟機器互動（這是趨勢，無論是智慧型吸塵器或人形機器人護士），它們必須要有更多能耐，而不只是了解我們說的話：我們還必須讓它們懂我們，換句話說，我們必須讓它們能夠「了解」並分享人類情緒，也就是擁有同理心。
我在香港科技大學的實驗室裡正在發展這類機器。有同理心的機器人可能對社會大有幫助，它們將不只是助理，也會是同伴。它們會很友善，讓人感覺溫暖，能預料我們在生理和情緒上的需要；它們能從跟人類的互動中學習；它們能讓我們的生活更美好、工作更有效率；它們會為錯誤道歉，並在行動之前請求同意；它們能照顧長者、教導孩童，甚至能在緊急情況時，展現出最高的同理心，不惜犧牲自己也要拯救你的生命。
能模仿情緒的機器人已經問世，其中包括Pepper和Jibo。Pepper是法國的隨從機器人學公司（AldebaranRobotics）為日本的軟體銀行公司（原名SoftBankMobile，2015年7月改名為SoftBank）打造的小型人形機器人；Jibo則是約2.7公斤重的桌上型個人助理機器人，參與設計的工程師包括語音成果國際公司對話技術部門的前主管佩拉奇尼（RobertoPieraccini）。同理心機器人這領域才剛起步，但能大幅改善這些機器的工具和演算法，已經浮現。
開發同理心模組
我從六年前開始想要打造同理心機器人，當時我的研究小組設計了第一個相當於中文版的Siri，我觀察到使用者對個人助理系統會自然發展出情緒反應，而且當機器無法了解使用者想溝通的事情，使用者會變得沮喪，我對此深感興趣。我了解到，要打造能了解人類情緒的機器，關鍵在於語音辨識演算法，而我已鑽研這項技術25年。
任何智慧型機器的核心，都是由模組構成的軟體系統，每個模組都是執行單一任務的程式。智慧型機器人可能會有負責處理人類語音、辨識攝影機拍攝到的影像等多項模組。有同理心的機器人會有一顆心，也就是名為「同理心模組」（empathymodule）的軟體。同理心模組會分析表情線索、語音中的聲學標記，以及語音本身的內容，才能分析人類的情緒，好讓機器人知道該如何反應。
兩個人在溝通時，會自動使用各種線索去了解對方的情緒狀態——他們會分析臉部表情和肢體語言、察覺聲調變化、了解語音的內容。要打造同理心模組，研究人員就必須先辨識出，在人類溝通中能夠讓機器用來辨識情緒的各類特徵，然後訓練演算法去找出這些特徵。
當我的研究小組開始訓練機器去偵測語音中的情緒，我們教機器辨認的不是只有語音本身的意義，還包括語音中的基本聲學特徵（acousticfeature），因為人類就是這樣做的。我們很少用這些名詞來思考，但人類的溝通其實就是訊號處理。人腦能偵測到某人聲音中的情緒，因為我們注意到了標示著緊張、歡樂、恐懼、憤怒、厭惡等情緒的聲學線索。我們在高興的時候，講話的速度會比較快，音調也會上揚；我們感受到壓力的時候，聲音會變得平板單調。利用訊號處理技術，電腦能偵測這些線索，就好像測謊儀會測出血壓、脈搏和皮膚導電率。為了偵測壓力，我們利用監督式學習（supervisedlearning）訓練「機器學習演算法」去辨識與壓力有關的聲音線索。
一段人類語音的簡短錄音，可能只包含幾個字，但我們能從聲調中擷取出大量的訊號處理資料。我們最初先教機器辨認敝校學生語音樣本中的負面壓力（苦惱），學生還因此幫學校取了個綽號：香港壓力緊張大學。我們詢問學生12個使壓力逐漸增強的問題，建立了史上第一個自然壓力情緒的多語言（英語、華語和粵語）語料庫，等到蒐集了大約10小時的資料後，我們的演算法對於辨識壓力已有70%的準確率，跟人類的聆聽者相當，是很傑出的表現。
在此同時，我研究小組裡的另一隊人馬，則在訓練機器辨認音樂中的氣氛，方法是單獨分析聲音特徵（sonicfeature，換句話說，不理會歌詞）。相較於情緒，氣氛是在音樂播放期間持續存在的氛圍。這隊研究人員一開始先從歐洲與亞洲的主要語言中，蒐集了5000首各種類型的音樂，其中幾百首已經由音樂學家分類成14種氣氛。
我們從每首歌裡擷取出大約1000種基本訊號屬性（例如能量、基本頻率、諧波等聲學參數），然後用這些分類好的音樂來訓練14種分類器（classifier）軟體，每種分類器判別一首歌是否屬於某種特定氣氛。例如某種分類器只判別快樂的音樂，而另一種分類器只判別憂鬱的音樂。這14種分類器會以彼此的猜測為基礎，相互合作。如果「快樂」分類器錯把憂鬱的歌曲歸類為快樂，那麼在下一輪的重新學習中，這種分類器就必須重新訓練；每一輪，最差的分類器都必須重新訓練，這樣整體系統才會大幅進步。依照這種方法，讓機器「聆聽」大量音樂，並學習哪首歌屬於哪種氣氛，假以時日，機器就能跟我們大多數人一樣，只要聽到聲音，就能分辨任何一首歌的氣氛。以這項研究為基礎，我和以前的學生創辦了知音科技公司（IvoTechnologies），專門開發家用型的同理心機器人。第一項產品Moodbox是一款智慧型居家資訊娛樂中心，能控制每個房間的音樂和燈光，並回應使用者的情緒。
