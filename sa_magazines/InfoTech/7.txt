教機器人說不 : 先別擔心機器人會違抗人類命令，因為不懷好意的主人與誤判指令將造成更大威脅。
機器人必須遵守人類命令，這道理似乎再簡單不過。科幻小說家艾西莫夫（IsaacAsimov）便把服從人類當做他著名的機器人三法則的基礎。但仔細想想，盲目聽從別人告訴你怎麼做而不管後果，真的是個好主意？當然不是。對機器來說也是如此，尤其機器人可能會按照字面解讀人類命令而不考慮後果。
如果機器人接收的命令牴觸三法則之一「機器人不得傷害人類，或袖手旁觀使人類受傷」，連艾西莫夫都允許它不必聽從主人。此外，他還認為「機器人必須保護自己」，除非這麼做會傷害人類，或是直接反抗人類命令。隨著機器人與智慧機器成為精密而昂貴的人類資產，不論艾西莫夫的法則或常識都告訴我們，如果機器人收到的命令可能會傷害它們自身或周遭人事物，甚至危及主人的安全，機器人必須有能力質疑是否該執行這道命令。
試想家務機器人聽從命令去廚房拿橄欖油，然後回到餐桌淋在沙拉上。忙碌的主人沒注意到機器人還在廚房，便下令倒油，結果機器人把油倒在滾燙的爐子上引起火災。
再試想照護機器人陪伴老婦人去公園，婦人坐在長椅上打瞌睡。當她睡著後，有個惡作劇的人經過，要求機器人替他買披薩。被迫遵從人類命令的機器人立即離去、尋找披薩店，留下老婦人獨自在公園裡無人照料。
又例如在寒冷的冬天早晨，上班遲到的某人準備趕去公司參加重要會議。他搭乘用語音控制的自動車，下令前往辦公室。結冰的路面超出汽車循跡控制系統的負荷，因此自動駕駛系統減緩車速直到遠低於速限以策安全。但他正專心準備會議資料而未注意路況，要求自動車開快點。於是系統提高車速，此時輪胎行駛過一大片冰，導致車輛失控打滑並撞上對向來車。
學習拒絕命令
我們在實驗室中替一具實體機器人增加推論機制，協助它判斷可能的危險，或經過深思熟慮後才執行人類命令。研究使用的NAO機器人重4.3公斤、高58公分，配備攝影機與聲納感測器，能探測障礙物與其他危險，採用能加強自然語言與AI效能的特製軟體進行控制。
藉由探討語言學家所謂的「適切條件」，也就是根據情境因素決定個體是否應該做某件事，提供我們初始研究的概念架構。我們列出一份關於適切條件的檢查表，協助機器人決定是否遵從人類命令：我知不知道如何達成這項命令？我是否有能力達成這項命令？我現在有辦法達成這項命令嗎？根據我的社會角色或與下達指令的人的關係，我有必要達成這項命令嗎？達成這項命令是否會違反任何規範或道德原則，可能造成意外或不必要的傷害？我們把檢查表轉換成演算法，編譯入機器人的處理系統後開始實驗。
機器人透過一連串的語音、語言與對話處理器，把收到的簡單指令連結到簡單的推論機制。當我們告訴機器人「坐下」或「站起來」時，它會透過裝在頭部的喇叭回答「好的」、然後照做。但是當它靠近桌緣，聲納感測器探測到有摔落的危險時，便會止步不前：
機器人遲疑片刻，等處理器再次檢查過適切條件後，便踏過桌緣，落到人類同伴的手中。
教導機器人如何判斷適切條件，在可預見的未來，仍然是有待研究人員解決的複雜挑戰。這份演算法檢查表仰賴機器人熟悉各種社會角色與因果的概念，並能根據情況做出判斷。但像這類容易相信他人的機器人，並無法判斷感測器探測範圍之外的危險。如果不懷好意的人類故意欺騙機器人踏過桌緣，它可能會嚴重受損。這項實驗是充滿希望的第一步，教導機器人為了主人或自身著想而拒絕命令。
